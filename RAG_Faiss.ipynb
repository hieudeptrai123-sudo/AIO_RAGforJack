{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hieudeptrai123-sudo/AIO_RAGforJack/blob/main/RAG_Faiss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries\n"
      ],
      "metadata": {
        "id": "u6mlFC3Na2Z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hieudeptrai123-sudo/AIO_RAGforJack.git\n",
        "!mv /content/AIO_RAGforJack/LegalDataset.py /content"
      ],
      "metadata": {
        "id": "AZiKrVsLe3Gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvi transformers faiss-gpu torch datasets unidecode gdown"
      ],
      "metadata": {
        "collapsed": true,
        "id": "lIMTtcXUbAtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm #If this cause IProgree Error, change to tqdm instead of tqdm.notebook\n",
        "from transformers import RagRetriever, RagTokenForGeneration, DPRContextEncoder, DPRQuestionEncoder, DPRContextEncoderTokenizer, DPRQuestionEncoderTokenizer, AdamW\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from LegalDataset import LegalDataset\n",
        "import faiss\n",
        "from datasets import Dataset\n",
        "import time\n",
        "import gdown"
      ],
      "metadata": {
        "id": "DsdHmrYia63A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdown.download_folder('https://drive.google.com/drive/folders/1XpqF_ejSmQQJ4IsO38hJDZgMWLZelJyW?usp=sharing',quiet = False)"
      ],
      "metadata": {
        "id": "WiyBvYW1f1kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "OgO_5tV_co8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Function to free CUDA memory\n",
        "def free_memory():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "yM8fk-EfcsIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load dataset\n",
        "corpus_df = pd.read_csv('/content/AIO_RAGforJack/corpus.csv')  # Assuming columns: 'context' and 'cid'\n",
        "train_df = pd.read_csv('/content/AIO_RAGforJack/train.csv')  # Assuming columns: 'question', 'context', 'cid', 'qid'\n",
        "public_test_df = pd.read_csv('/content/AIO_RAGforJack/public_test.csv')  # Assuming columns: 'question', 'qid'\n",
        "\n",
        "# Step 2: Extract passages and their IDs\n",
        "passages = corpus_df['text'].tolist()\n",
        "passage_ids = corpus_df['cid'].tolist()"
      ],
      "metadata": {
        "id": "qLjkbKdvct-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Load DPR context and question encoders with appropriate tokenizers\n",
        "dpr_context_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\", ignore_mismatched_sizes=True).to(device)\n",
        "dpr_context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "dpr_question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\").to(device)\n",
        "dpr_question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")"
      ],
      "metadata": {
        "id": "p-nPIkeqcwce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Create a custom retriever using FAISS\n",
        "class FaissRetriever(RagRetriever):\n",
        "    def __init__(self, index, passages, tokenizer, top_k=5):\n",
        "        self.index = index\n",
        "        self.passages = passages\n",
        "        self.tokenizer = tokenizer\n",
        "        self.top_k = top_k\n",
        "\n",
        "    def retrieve(self, question_input_ids, question_hidden_states, question_attention_mask=None):\n",
        "        # Tokenize and embed the question using DPRQuestionEncoder\n",
        "        question_embeds = self._embed_question(question_input_ids)\n",
        "\n",
        "        # Search FAISS index for top-k relevant documents\n",
        "        _, indices = self.index.search(question_embeds.cpu().numpy(), self.top_k)  # FAISS on CPU\n",
        "\n",
        "        # Return top-k passages\n",
        "        retrieved_passages = [self.passages[i] for i in indices[0]]\n",
        "        return retrieved_passages\n",
        "\n",
        "    def _embed_question(self, question_input_ids):\n",
        "        question = self.tokenizer.batch_decode(question_input_ids, skip_special_tokens=True)\n",
        "        inputs = dpr_question_tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "\n",
        "        # Move inputs to the CUDA\n",
        "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            question_embeddings = dpr_question_encoder(**inputs).pooler_output\n",
        "        return question_embeddings"
      ],
      "metadata": {
        "id": "aJvrfnU0cx4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Embed all passages using DPRContextEncoder with memory management\n",
        "def embed_passages(passages, batch_size=32):\n",
        "    all_embeddings = []\n",
        "\n",
        "    # Wrap the for loop with tqdm to display the progress\n",
        "    for i in tqdm(range(0, len(passages), batch_size), desc=\"Embedding Passages\"):\n",
        "        batch = passages[i:i + batch_size]\n",
        "        inputs = dpr_context_tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "\n",
        "        # Move inputs to the CUDA\n",
        "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "        # Disable gradient computation for faster inference\n",
        "        with torch.no_grad():\n",
        "            embeddings = dpr_context_encoder(**inputs).pooler_output\n",
        "\n",
        "        # Move embeddings to CPU to free up CUDA memory\n",
        "        all_embeddings.append(embeddings.cpu())\n",
        "\n",
        "        # Free CUDA memory\n",
        "        free_memory()\n",
        "\n",
        "    # Concatenate all the embeddings into a single tensor\n",
        "    return torch.cat(all_embeddings, dim=0)"
      ],
      "metadata": {
        "id": "TaXyl3EUc0lM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get embeddings for all passages\n",
        "passage_embeddings = embed_passages(passages)\n",
        "free_memory()"
      ],
      "metadata": {
        "id": "lPsyyZL4c1OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Use FAISS to index the passage embeddings\n",
        "d = passage_embeddings.shape[1]  # Dimensionality of the embeddings\n",
        "\n",
        "# Create a FAISS index for inner product (dot product) on CPU\n",
        "index = faiss.IndexFlatIP(d)  # Inner Product index on CPU\n",
        "\n",
        "# Add embeddings to the index (no need to move to GPU)\n",
        "index.add(passage_embeddings.cpu().numpy())  # Add embeddings to the index\n",
        "\n",
        "# Optional: If you want to verify the number of embeddings added\n",
        "print(f\"Total passages indexed: {index.ntotal}\")\n",
        "\n",
        "# Clear memory if needed\n",
        "free_memory()"
      ],
      "metadata": {
        "id": "66gIjX52c2xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, optimizer, device, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\"):\n",
        "            # Move batch to device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            context_input_ids = batch['context_input_ids'].to(device)\n",
        "            context_attention_mask = batch['context_attention_mask'].to(device)\n",
        "\n",
        "            # Print shapes of input tensors for debugging\n",
        "            print(\"Input IDs shape:\", input_ids.shape)\n",
        "            print(\"Attention mask shape:\", attention_mask.shape)\n",
        "            print(\"Context input IDs shape:\", context_input_ids.shape)\n",
        "            print(\"Context attention mask shape:\", context_attention_mask.shape)\n",
        "\n",
        "            # Check input shapes\n",
        "            try:\n",
        "                # Expected shape: [batch_size, sequence_length]\n",
        "                expected_shape = (input_ids.size(0), input_ids.size(1))  # (batch_size, seq_length)\n",
        "                assert input_ids.dim() == 2, \"Input IDs should be 2-dimensional\"\n",
        "                assert input_ids.shape == expected_shape, f\"Expected input_ids shape {expected_shape}, but got {input_ids.shape}\"\n",
        "\n",
        "                assert attention_mask.dim() == 2, \"Attention mask should be 2-dimensional\"\n",
        "                assert attention_mask.shape == expected_shape, f\"Expected attention_mask shape {expected_shape}, but got {attention_mask.shape}\"\n",
        "\n",
        "                assert context_input_ids.dim() == 2, \"Context input IDs should be 2-dimensional\"\n",
        "                assert context_input_ids.shape == expected_shape, f\"Expected context_input_ids shape {expected_shape}, but got {context_input_ids.shape}\"\n",
        "\n",
        "                assert context_attention_mask.dim() == 2, \"Context attention mask should be 2-dimensional\"\n",
        "                assert context_attention_mask.shape == expected_shape, f\"Expected context_attention_mask shape {expected_shape}, but got {context_attention_mask.shape}\"\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    context_input_ids=context_input_ids,\n",
        "                    context_attention_mask=context_attention_mask,\n",
        "                    labels=context_input_ids\n",
        "                )\n",
        "\n",
        "                # Process loss and perform backpropagation\n",
        "                loss = outputs.loss\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            except AssertionError as e:\n",
        "                print(f\"Shape Assertion Error: {e}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error during forward pass: {e}\")\n",
        "\n",
        "        print(f\"Epoch {epoch + 1} completed. Loss: {epoch_loss / len(train_loader)}\")"
      ],
      "metadata": {
        "id": "LA6PevHNc4Zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full training script with updated function\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Assuming train_df is already defined and preprocessed\n",
        "    train_dataset = LegalDataset(\n",
        "        df=train_df,\n",
        "        tokenizer_question=dpr_question_tokenizer,\n",
        "        tokenizer_context=dpr_context_tokenizer,\n",
        "        max_length=128  # Maximum token length for truncation\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=8,\n",
        "        shuffle=True,\n",
        "        num_workers=4\n",
        "    )\n",
        "\n",
        "    retriever = FaissRetriever(index=index, passages=passages, tokenizer=dpr_question_tokenizer, top_k=5)\n",
        "    model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever).to(device)\n",
        "    free_memory\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "    model.train()\n",
        "    # Train the model\n",
        "    train_model(model, train_loader, optimizer, device, num_epochs=50)\n",
        "\n",
        "    # Save the fine-tuned model\n",
        "    model.save_pretrained('fine_tuned_rag_model')\n",
        "    print(\"Fine-tuning completed and model saved.\")"
      ],
      "metadata": {
        "id": "YadEbkoVc6Ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ],
      "metadata": {
        "id": "wEx1HqoMc9nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Loading the fine-tuned model\n",
        "print(\"Loading the fine-tuned model...\")\n",
        "retriever = FaissRetriever(index=index, passages=passages, tokenizer=dpr_question_tokenizer, top_k=10)\n",
        "fine_tuned_model = RagTokenForGeneration.from_pretrained('fine_tuned_rag_model', retriever=retriever).to(device)\n",
        "print(\"Fine-tuned model loaded successfully.\")\n",
        "# Step 8: Predict top-k documents for each query in public_test.csv with progress bar\n",
        "top_k_predictions = []\n",
        "fine_tuned_model.eval()\n",
        "# Use tqdm to display progress while generating predictions\n",
        "for idx, row in tqdm(public_test_df.iterrows(), total=len(public_test_df), desc=\"Processing Queries\"):\n",
        "    question = row['question']\n",
        "    qid = row['qid']\n",
        "\n",
        "    # Tokenize question\n",
        "    input_ids = dpr_question_tokenizer(question, return_tensors=\"pt\", max_length=128, truncation=True, padding=True).input_ids\n",
        "\n",
        "    # Move inputs to the MPS\n",
        "    input_ids = input_ids.to(device)\n",
        "\n",
        "    # Generate top-k passage predictions\n",
        "    retrieved_passages = retriever.retrieve(input_ids, None)\n",
        "\n",
        "    # Retrieve the passage IDs (cid) for the predicted passages\n",
        "    retrieved_cids = [corpus_df.loc[corpus_df['text'] == passage]['cid'].values[0] for passage in retrieved_passages]\n",
        "\n",
        "    # Append result (qid followed by top-k cids)\n",
        "    top_k_predictions.append(f\"{qid} \" + \" \".join(map(str, retrieved_cids)))\n",
        "\n",
        "    # Free MPS memory after each query\n",
        "    free_memory()\n",
        "\n",
        "# Step 9: Save predictions to predict.txt\n",
        "with open('predict.txt', 'w') as f:\n",
        "    for prediction in top_k_predictions:\n",
        "        f.write(prediction + '\\n')\n",
        "\n",
        "print(\"Predictions saved to predict.txt.\")"
      ],
      "metadata": {
        "id": "Z7nWSDD1c_T9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
