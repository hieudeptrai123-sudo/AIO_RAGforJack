{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9H5R0pO9AU5UA/xhjvZD1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hieudeptrai123-sudo/AIO_RAGforJack/blob/main/Untitled13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries\n"
      ],
      "metadata": {
        "id": "u6mlFC3Na2Z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvi transformers faiss-gpu torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lIMTtcXUbAtk",
        "outputId": "de75aa96-c5c6-47be-8dc0-ecbc28e30a51"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyvi\n",
            "  Downloading pyvi-0.1.1-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pyvi) (1.5.2)\n",
            "Collecting sklearn-crfsuite (from pyvi)\n",
            "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (3.5.0)\n",
            "Collecting python-crfsuite>=0.9.7 (from sklearn-crfsuite->pyvi)\n",
            "  Downloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
            "Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu, python-crfsuite, sklearn-crfsuite, pyvi\n",
            "Successfully installed faiss-gpu-1.7.2 python-crfsuite-0.9.11 pyvi-0.1.1 sklearn-crfsuite-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyvi import ViTokenizer\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import RagRetriever, RagTokenForGeneration, DPRContextEncoder, DPRQuestionEncoder, DPRContextEncoderTokenizer, DPRQuestionEncoderTokenizer, AdamW\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from LegalDataset import LegalDataset\n",
        "import faiss\n",
        "from multiprocessing import Pool\n",
        "import time"
      ],
      "metadata": {
        "id": "DsdHmrYia63A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "QzPhqwr7cXWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = pd.read_csv('corpus.csv')\n",
        "public_test = pd.read_csv('public_test.csv')\n",
        "train = pd.read_csv('train.csv')"
      ],
      "metadata": {
        "id": "4O8K35N4cZ9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows with NaN values and reset index of corpus dataframe\n",
        "corpus = corpus.dropna().reset_index(drop=True)\n",
        "\n",
        "# Remove rows with NaN values and reset index of public_test dataframe\n",
        "public_test = public_test.dropna().reset_index(drop=True)\n",
        "\n",
        "# Remove rows with NaN values and reset index of train dataframe\n",
        "train = train.dropna().reset_index(drop=True)"
      ],
      "metadata": {
        "id": "cWX9g6Uxccm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus['text'] = corpus['text'].apply(lambda x: x.replace('\\n', ' '))\n",
        "corpus['text'] = corpus['text'].apply(lambda x: x.replace('\\t', ' '))\n",
        "corpus['text'] = corpus['text'].apply(lambda x: x.replace('\\r', ' '))\n",
        "corpus['text'] = corpus['text'].apply(lambda x: x.replace('  ', ' '))\n",
        "corpus['text'] = corpus['text'].apply(lambda x: x.lower())\n",
        "\n",
        "# Đường dẫn đến hai file .txt\n",
        "file1_path = 'vietnamese-stopwords-dash.txt'  # Thay đổi đường dẫn tới file thứ nhất\n",
        "file2_path = 'vietnamese-stopwords.txt'  # Thay đổi đường dẫn tới file thứ hai\n",
        "def read_stopwords(file_path):\n",
        "    stopwords = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            stopword = line.strip()  # Tách theo khoảng trắng\n",
        "            stopwords.append(stopword)  # Thêm stopword vào list\n",
        "    return stopwords\n",
        "stopwords1 = read_stopwords(file1_path)\n",
        "stopwords2 = read_stopwords(file2_path)\n",
        "\n",
        "vietnamese_stopwords = set(stopwords1 + stopwords2)\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    no_punct_text = re.sub(r'[^\\w\\s]', '', text)  # This will keep Vietnamese letters and spaces\n",
        "    return no_punct_text\n",
        "\n",
        "corpus['text'] = corpus['text'].apply(lambda x: ViTokenizer.tokenize(x))\n",
        "\n",
        "corpus['text'] = corpus['text'].apply(lambda x: remove_punctuation(x))\n",
        "\n",
        "corpus['text'] = corpus['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in vietnamese_stopwords]))\n",
        "\n",
        "corpus['text'] = corpus['text'].apply(lambda x: unidecode.unidecode(x))\n",
        "public_test['question'] = public_test['question'].apply(lambda x: x.replace('\\n', ' '))\n",
        "public_test['question'] = public_test['question'].apply(lambda x: x.replace('\\t', ' '))\n",
        "public_test['question'] = public_test['question'].apply(lambda x: x.replace('\\r', ' '))\n",
        "public_test['question'] = public_test['question'].apply(lambda x: x.replace('  ', ' '))\n",
        "public_test['question'] = public_test['question'].apply(lambda x: x.lower())\n",
        "\n",
        "# Đường dẫn đến hai file .txt\n",
        "file1_path = 'vietnamese-stopwords-dash.txt'  # Thay đổi đường dẫn tới file thứ nhất\n",
        "file2_path = 'vietnamese-stopwords.txt'  # Thay đổi đường dẫn tới file thứ hai\n",
        "def read_stopwords(file_path):\n",
        "    stopwords = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            stopword = line.strip()  # Tách theo khoảng trắng\n",
        "            stopwords.append(stopword)  # Thêm stopword vào list\n",
        "    return stopwords\n",
        "stopwords1 = read_stopwords(file1_path)\n",
        "stopwords2 = read_stopwords(file2_path)\n",
        "\n",
        "vietnamese_stopwords = set(stopwords1 + stopwords2)\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    no_punct_text = re.sub(r'[^\\w\\s]', '', text)  # This will keep Vietnamese letters and spaces\n",
        "    return no_punct_text\n",
        "\n",
        "public_test['question'] = public_test['question'].apply(lambda x: ViTokenizer.tokenize(x))\n",
        "\n",
        "public_test['question'] = public_test['question'].apply(lambda x: remove_punctuation(x))\n",
        "\n",
        "public_test['question'] = public_test['question'].apply(lambda x: ' '.join([word for word in x.split() if word not in vietnamese_stopwords]))\n",
        "\n",
        "public_test['question'] = public_test['question'].apply(lambda x: unidecode.unidecode(x))\n",
        "train['question'] = train['question'].apply(lambda x: x.replace('\\n', ' '))\n",
        "train['question'] = train['question'].apply(lambda x: x.replace('\\t', ' '))\n",
        "train['question'] = train['question'].apply(lambda x: x.replace('\\r', ' '))\n",
        "train['question'] = train['question'].apply(lambda x: x.replace('  ', ' '))\n",
        "train['question'] = train['question'].apply(lambda x: x.lower())\n",
        "\n",
        "# Đường dẫn đến hai file .txt\n",
        "file1_path = 'vietnamese-stopwords-dash.txt'  # Thay đổi đường dẫn tới file thứ nhất\n",
        "file2_path = 'vietnamese-stopwords.txt'  # Thay đổi đường dẫn tới file thứ hai\n",
        "def read_stopwords(file_path):\n",
        "    stopwords = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            stopword = line.strip()  # Tách theo khoảng trắng\n",
        "            stopwords.append(stopword)  # Thêm stopword vào list\n",
        "    return stopwords\n",
        "stopwords1 = read_stopwords(file1_path)\n",
        "stopwords2 = read_stopwords(file2_path)\n",
        "\n",
        "vietnamese_stopwords = set(stopwords1 + stopwords2)\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    no_punct_text = re.sub(r'[^\\w\\s]', '', text)  # This will keep Vietnamese letters and spaces\n",
        "    return no_punct_text\n",
        "\n",
        "train['question'] = train['question'].apply(lambda x: ViTokenizer.tokenize(x))\n",
        "\n",
        "train['question'] = train['question'].apply(lambda x: remove_punctuation(x))\n",
        "\n",
        "train['question'] = train['question'].apply(lambda x: ' '.join([word for word in x.split() if word not in vietnamese_stopwords]))\n",
        "\n",
        "train['question'] = train['question'].apply(lambda x: unidecode.unidecode(x))\n",
        "train['context'] = train['context'].apply(lambda x: x.replace('\\n', ' '))\n",
        "train['context'] = train['context'].apply(lambda x: x.replace('\\t', ' '))\n",
        "train['context'] = train['context'].apply(lambda x: x.replace('\\r', ' '))\n",
        "train['context'] = train['context'].apply(lambda x: x.replace('  ', ' '))\n",
        "train['context'] = train['context'].apply(lambda x: x.lower())\n",
        "\n",
        "# Đường dẫn đến hai file .txt\n",
        "file1_path = 'vietnamese-stopwords-dash.txt'  # Thay đổi đường dẫn tới file thứ nhất\n",
        "file2_path = 'vietnamese-stopwords.txt'  # Thay đổi đường dẫn tới file thứ hai\n",
        "def read_stopwords(file_path):\n",
        "    stopwords = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            stopword = line.strip()  # Tách theo khoảng trắng\n",
        "            stopwords.append(stopword)  # Thêm stopword vào list\n",
        "    return stopwords\n",
        "stopwords1 = read_stopwords(file1_path)\n",
        "stopwords2 = read_stopwords(file2_path)\n",
        "\n",
        "vietnamese_stopwords = set(stopwords1 + stopwords2)\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    no_punct_text = re.sub(r'[^\\w\\s]', '', text)  # This will keep Vietnamese letters and spaces\n",
        "    return no_punct_text\n",
        "\n",
        "train['context'] = train['context'].apply(lambda x: ViTokenizer.tokenize(x))\n",
        "\n",
        "train['context'] = train['context'].apply(lambda x: remove_punctuation(x))\n",
        "\n",
        "train['context'] = train['context'].apply(lambda x: ' '.join([word for word in x.split() if word not in vietnamese_stopwords]))\n",
        "\n",
        "train['context'] = train['context'].apply(lambda x: unidecode.unidecode(x))"
      ],
      "metadata": {
        "id": "IX_hd9M0ceLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.to_csv('train.csv', index=False)\n",
        "public_test.to_csv('public_test.csv', index=False)\n",
        "corpus.to_csv('corpus.csv', index=False)"
      ],
      "metadata": {
        "id": "m3JNnEFNcgQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "OgO_5tV_co8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.backends.cuda.is_available() else \"cpu\")\n",
        "# Function to free CUDA memory\n",
        "def free_memory():\n",
        "    if torch.backends.cuda.is_available():\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "yM8fk-EfcsIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load dataset\n",
        "corpus_df = pd.read_csv('corpus.csv')  # Assuming columns: 'context' and 'cid'\n",
        "train_df = pd.read_csv('train.csv')  # Assuming columns: 'question', 'context', 'cid', 'qid'\n",
        "public_test_df = pd.read_csv('public_test.csv')  # Assuming columns: 'question', 'qid'\n",
        "\n",
        "# Step 2: Extract passages and their IDs\n",
        "passages = corpus_df['text'].tolist()\n",
        "passage_ids = corpus_df['cid'].tolist()"
      ],
      "metadata": {
        "id": "qLjkbKdvct-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Load DPR context and question encoders with appropriate tokenizers\n",
        "dpr_context_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\", ignore_mismatched_sizes=True).to(device)\n",
        "dpr_context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "dpr_question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\").to(device)\n",
        "dpr_question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")"
      ],
      "metadata": {
        "id": "p-nPIkeqcwce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Create a custom retriever using FAISS\n",
        "class FaissRetriever(RagRetriever):\n",
        "    def __init__(self, index, passages, tokenizer, top_k=5):\n",
        "        self.index = index\n",
        "        self.passages = passages\n",
        "        self.tokenizer = tokenizer\n",
        "        self.top_k = top_k\n",
        "\n",
        "    def retrieve(self, question_input_ids, question_hidden_states, question_attention_mask=None):\n",
        "        # Tokenize and embed the question using DPRQuestionEncoder\n",
        "        question_embeds = self._embed_question(question_input_ids)\n",
        "\n",
        "        # Search FAISS index for top-k relevant documents\n",
        "        _, indices = self.index.search(question_embeds.cpu().numpy(), self.top_k)  # FAISS on CPU\n",
        "\n",
        "        # Return top-k passages\n",
        "        retrieved_passages = [self.passages[i] for i in indices[0]]\n",
        "        return retrieved_passages\n",
        "\n",
        "    def _embed_question(self, question_input_ids):\n",
        "        question = self.tokenizer.batch_decode(question_input_ids, skip_special_tokens=True)\n",
        "        inputs = dpr_question_tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "\n",
        "        # Move inputs to the CUDA\n",
        "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            question_embeddings = dpr_question_encoder(**inputs).pooler_output\n",
        "        return question_embeddings"
      ],
      "metadata": {
        "id": "aJvrfnU0cx4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Embed all passages using DPRContextEncoder with memory management\n",
        "def embed_passages(passages, batch_size=32):\n",
        "    all_embeddings = []\n",
        "\n",
        "    # Wrap the for loop with tqdm to display the progress\n",
        "    for i in tqdm(range(0, len(passages), batch_size), desc=\"Embedding Passages\"):\n",
        "        batch = passages[i:i + batch_size]\n",
        "        inputs = dpr_context_tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "\n",
        "        # Move inputs to the CUDA\n",
        "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "        # Disable gradient computation for faster inference\n",
        "        with torch.no_grad():\n",
        "            embeddings = dpr_context_encoder(**inputs).pooler_output\n",
        "\n",
        "        # Move embeddings to CPU to free up CUDA memory\n",
        "        all_embeddings.append(embeddings.cpu())\n",
        "\n",
        "        # Free CUDA memory\n",
        "        free_memory()\n",
        "\n",
        "    # Concatenate all the embeddings into a single tensor\n",
        "    return torch.cat(all_embeddings, dim=0)"
      ],
      "metadata": {
        "id": "TaXyl3EUc0lM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get embeddings for all passages\n",
        "passage_embeddings = embed_passages(passages)\n",
        "free_memory()"
      ],
      "metadata": {
        "id": "lPsyyZL4c1OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Use FAISS to index the passage embeddings\n",
        "d = passage_embeddings.shape[1]  # Dimensionality of the embeddings\n",
        "\n",
        "# Create a FAISS index for inner product (dot product) on CPU\n",
        "index = faiss.IndexFlatIP(d)  # Inner Product index on CPU\n",
        "\n",
        "# Add embeddings to the index (no need to move to GPU)\n",
        "index.add(passage_embeddings.cpu().numpy())  # Add embeddings to the index\n",
        "\n",
        "# Optional: If you want to verify the number of embeddings added\n",
        "print(f\"Total passages indexed: {index.ntotal}\")\n",
        "\n",
        "# Clear memory if needed\n",
        "free_memory()"
      ],
      "metadata": {
        "id": "66gIjX52c2xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, optimizer, device, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\"):\n",
        "            # Move batch to device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            context_input_ids = batch['context_input_ids'].to(device)\n",
        "            context_attention_mask = batch['context_attention_mask'].to(device)\n",
        "\n",
        "            # Print shapes of input tensors for debugging\n",
        "            print(\"Input IDs shape:\", input_ids.shape)\n",
        "            print(\"Attention mask shape:\", attention_mask.shape)\n",
        "            print(\"Context input IDs shape:\", context_input_ids.shape)\n",
        "            print(\"Context attention mask shape:\", context_attention_mask.shape)\n",
        "\n",
        "            # Check input shapes\n",
        "            try:\n",
        "                # Expected shape: [batch_size, sequence_length]\n",
        "                expected_shape = (input_ids.size(0), input_ids.size(1))  # (batch_size, seq_length)\n",
        "                assert input_ids.dim() == 2, \"Input IDs should be 2-dimensional\"\n",
        "                assert input_ids.shape == expected_shape, f\"Expected input_ids shape {expected_shape}, but got {input_ids.shape}\"\n",
        "\n",
        "                assert attention_mask.dim() == 2, \"Attention mask should be 2-dimensional\"\n",
        "                assert attention_mask.shape == expected_shape, f\"Expected attention_mask shape {expected_shape}, but got {attention_mask.shape}\"\n",
        "\n",
        "                assert context_input_ids.dim() == 2, \"Context input IDs should be 2-dimensional\"\n",
        "                assert context_input_ids.shape == expected_shape, f\"Expected context_input_ids shape {expected_shape}, but got {context_input_ids.shape}\"\n",
        "\n",
        "                assert context_attention_mask.dim() == 2, \"Context attention mask should be 2-dimensional\"\n",
        "                assert context_attention_mask.shape == expected_shape, f\"Expected context_attention_mask shape {expected_shape}, but got {context_attention_mask.shape}\"\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    context_input_ids=context_input_ids,\n",
        "                    context_attention_mask=context_attention_mask,\n",
        "                    labels=context_input_ids\n",
        "                )\n",
        "\n",
        "                # Process loss and perform backpropagation\n",
        "                loss = outputs.loss\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            except AssertionError as e:\n",
        "                print(f\"Shape Assertion Error: {e}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error during forward pass: {e}\")\n",
        "\n",
        "        print(f\"Epoch {epoch + 1} completed. Loss: {epoch_loss / len(train_loader)}\")"
      ],
      "metadata": {
        "id": "LA6PevHNc4Zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full training script with updated function\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Assuming train_df is already defined and preprocessed\n",
        "    train_dataset = LegalDataset(\n",
        "        df=train_df,\n",
        "        tokenizer_question=dpr_question_tokenizer,\n",
        "        tokenizer_context=dpr_context_tokenizer,\n",
        "        max_length=128  # Maximum token length for truncation\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=8,\n",
        "        shuffle=True,\n",
        "        num_workers=4\n",
        "    )\n",
        "\n",
        "    retriever = FaissRetriever(index=index, passages=passages, tokenizer=dpr_question_tokenizer, top_k=5)\n",
        "    model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever).to(device)\n",
        "    free_memory\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "    model.train()\n",
        "    # Train the model\n",
        "    train_model(model, train_loader, optimizer, device, num_epochs=50)\n",
        "\n",
        "    # Save the fine-tuned model\n",
        "    model.save_pretrained('fine_tuned_rag_model')\n",
        "    print(\"Fine-tuning completed and model saved.\")"
      ],
      "metadata": {
        "id": "YadEbkoVc6Ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ],
      "metadata": {
        "id": "wEx1HqoMc9nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Loading the fine-tuned model\n",
        "print(\"Loading the fine-tuned model...\")\n",
        "retriever = FaissRetriever(index=index, passages=passages, tokenizer=dpr_question_tokenizer, top_k=10)\n",
        "fine_tuned_model = RagTokenForGeneration.from_pretrained('fine_tuned_rag_model', retriever=retriever).to(device)\n",
        "print(\"Fine-tuned model loaded successfully.\")\n",
        "# Step 7: Initialize the custom retriever and the RAG model\n",
        "retriever = FaissRetriever(index=index, passages=passages, tokenizer=dpr_question_tokenizer, top_k=10)\n",
        "model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever).to(device)\n",
        "\n",
        "# Step 8: Predict top-k documents for each query in public_test.csv with progress bar\n",
        "top_k_predictions = []\n",
        "fine_tuned_model.eval()\n",
        "# Use tqdm to display progress while generating predictions\n",
        "for idx, row in tqdm(public_test_df.iterrows(), total=len(public_test_df), desc=\"Processing Queries\"):\n",
        "    question = row['question']\n",
        "    qid = row['qid']\n",
        "\n",
        "    # Tokenize question\n",
        "    input_ids = dpr_question_tokenizer(question, return_tensors=\"pt\", max_length=128, truncation=True, padding=True).input_ids\n",
        "\n",
        "    # Move inputs to the MPS\n",
        "    input_ids = input_ids.to(device)\n",
        "\n",
        "    # Generate top-k passage predictions\n",
        "    retrieved_passages = retriever.retrieve(input_ids, None)\n",
        "\n",
        "    # Retrieve the passage IDs (cid) for the predicted passages\n",
        "    retrieved_cids = [corpus_df.loc[corpus_df['text'] == passage]['cid'].values[0] for passage in retrieved_passages]\n",
        "\n",
        "    # Append result (qid followed by top-k cids)\n",
        "    top_k_predictions.append(f\"{qid} \" + \" \".join(map(str, retrieved_cids)))\n",
        "\n",
        "    # Free MPS memory after each query\n",
        "    free_memory()\n",
        "\n",
        "# Step 9: Save predictions to predict.txt\n",
        "with open('predict.txt', 'w') as f:\n",
        "    for prediction in top_k_predictions:\n",
        "        f.write(prediction + '\\n')\n",
        "\n",
        "print(\"Predictions saved to predict.txt.\")"
      ],
      "metadata": {
        "id": "Z7nWSDD1c_T9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}