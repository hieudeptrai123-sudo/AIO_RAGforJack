{"metadata":{"colab":{"provenance":[],"include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9725558,"sourceType":"datasetVersion","datasetId":5950978}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/hieunguyen2208/rag-faiss?scriptVersionId=204120589\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Import Libraries\n","metadata":{"id":"u6mlFC3Na2Z4"}},{"cell_type":"code","source":"!git clone https://github.com/hieudeptrai123-sudo/AIO_RAGforJack.git","metadata":{"id":"AZiKrVsLe3Gs","execution":{"iopub.status.busy":"2024-10-29T13:09:19.365681Z","iopub.execute_input":"2024-10-29T13:09:19.366362Z","iopub.status.idle":"2024-10-29T13:09:20.937522Z","shell.execute_reply.started":"2024-10-29T13:09:19.366317Z","shell.execute_reply":"2024-10-29T13:09:20.936587Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'AIO_RAGforJack'...\nremote: Enumerating objects: 215, done.\u001b[K\nremote: Counting objects: 100% (208/208), done.\u001b[K\nremote: Compressing objects: 100% (142/142), done.\u001b[K\nremote: Total 215 (delta 122), reused 119 (delta 64), pack-reused 7 (from 1)\u001b[K\nReceiving objects: 100% (215/215), 93.96 KiB | 4.27 MiB/s, done.\nResolving deltas: 100% (122/122), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!mv /kaggle/working/AIO_RAGforJack/LegalDataset.py /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2024-10-29T13:09:24.636992Z","iopub.execute_input":"2024-10-29T13:09:24.637975Z","iopub.status.idle":"2024-10-29T13:09:25.660442Z","shell.execute_reply.started":"2024-10-29T13:09:24.63793Z","shell.execute_reply":"2024-10-29T13:09:25.658478Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install transformers faiss-gpu torch datasets tqdm ","metadata":{"id":"lIMTtcXUbAtk","execution":{"iopub.status.busy":"2024-10-29T13:09:27.96157Z","iopub.execute_input":"2024-10-29T13:09:27.961989Z","iopub.status.idle":"2024-10-29T13:09:43.463544Z","shell.execute_reply.started":"2024-10-29T13:09:27.961952Z","shell.execute_reply":"2024-10-29T13:09:43.462416Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nCollecting faiss-gpu\n  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm #If this cause IProgree Error, change to tqdm instead of tqdm.notebook\nfrom transformers import RagRetriever, RagTokenForGeneration, DPRContextEncoder, DPRQuestionEncoder, DPRContextEncoderTokenizer, DPRQuestionEncoderTokenizer, AdamW\nimport torch\nfrom torch.utils.data import DataLoader\nfrom LegalDataset import LegalDataset\nimport faiss\nfrom torch.utils.data import Dataset\nimport time","metadata":{"id":"DsdHmrYia63A","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{"id":"OgO_5tV_co8y"}},{"cell_type":"code","source":"# Check if CUDA is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Function to free CUDA memory\ndef free_memory():\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()","metadata":{"id":"yM8fk-EfcsIl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 1: Load dataset\ncorpus_df = pd.read_csv('/kaggle/input/bkai-2024/corpus.csv')  # Assuming columns: 'context' and 'cid'\ntrain_df = pd.read_csv('/kaggle/input/bkai-2024/train.csv')  # Assuming columns: 'question', 'context', 'cid', 'qid'\npublic_test_df = pd.read_csv('/kaggle/input/bkai-2024/public_test.csv')  # Assuming columns: 'question', 'qid'\n\n# Step 2: Extract passages and their IDs\npassages = corpus_df['text'].tolist()\npassage_ids = corpus_df['cid'].tolist()","metadata":{"id":"qLjkbKdvct-D","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 3: Load DPR context and question encoders with appropriate tokenizers\ndpr_context_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\", ignore_mismatched_sizes=True).to(device)\ndpr_context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\ndpr_question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\").to(device)\ndpr_question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")","metadata":{"id":"p-nPIkeqcwce","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 4: Create a custom retriever using FAISS\nclass FaissRetriever(RagRetriever):\n    def __init__(self, index, passages, tokenizer, top_k=5):\n        self.index = index\n        self.passages = passages\n        self.tokenizer = tokenizer\n        self.top_k = top_k\n\n    def retrieve(self, question_input_ids, question_hidden_states, question_attention_mask=None):\n        # Tokenize and embed the question using DPRQuestionEncoder\n        question_embeds = self._embed_question(question_input_ids)\n\n        # Search FAISS index for top-k relevant documents\n        _, indices = self.index.search(question_embeds.cpu().numpy(), self.top_k)  # FAISS on CPU\n\n        # Return top-k passages\n        retrieved_passages = [self.passages[i] for i in indices[0]]\n        return retrieved_passages\n\n    def _embed_question(self, question_input_ids):\n        question = self.tokenizer.batch_decode(question_input_ids, skip_special_tokens=True)\n        inputs = dpr_question_tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n\n        # Move inputs to the CUDA\n        inputs = {key: value.to(device) for key, value in inputs.items()}\n\n        with torch.no_grad():\n            question_embeddings = dpr_question_encoder(**inputs).pooler_output\n        return question_embeddings","metadata":{"id":"aJvrfnU0cx4f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 5: Embed all passages using DPRContextEncoder with memory management\ndef embed_passages(passages, batch_size=32):\n    all_embeddings = []\n\n    # Wrap the for loop with tqdm to display the progress\n    for i in tqdm(range(0, len(passages), batch_size), desc=\"Embedding Passages\"):\n        batch = passages[i:i + batch_size]\n        inputs = dpr_context_tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n\n        # Move inputs to the CUDA\n        inputs = {key: value.to(device) for key, value in inputs.items()}\n\n        # Disable gradient computation for faster inference\n        with torch.no_grad():\n            embeddings = dpr_context_encoder(**inputs).pooler_output\n\n        # Move embeddings to CPU to free up CUDA memory\n        all_embeddings.append(embeddings.cpu())\n\n        # Free CUDA memory\n        free_memory()\n\n    # Concatenate all the embeddings into a single tensor\n    return torch.cat(all_embeddings, dim=0)","metadata":{"id":"TaXyl3EUc0lM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get embeddings for all passages\npassage_embeddings = embed_passages(passages)\nfree_memory()","metadata":{"id":"lPsyyZL4c1OO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 6: Use FAISS to index the passage embeddings\nd = passage_embeddings.shape[1]  # Dimensionality of the embeddings\n\n# Create a FAISS index for inner product (dot product) on CPU\nindex = faiss.IndexFlatIP(d)  # Inner Product index on CPU\n\n# Add embeddings to the index (no need to move to GPU)\nindex.add(passage_embeddings.cpu().numpy())  # Add embeddings to the index\n\n# Optional: If you want to verify the number of embeddings added\nprint(f\"Total passages indexed: {index.ntotal}\")\n\n# Clear memory if needed\nfree_memory()","metadata":{"id":"66gIjX52c2xW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, train_loader, optimizer, device, num_epochs):\n    for epoch in range(num_epochs):\n        epoch_loss = 0\n        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\"):\n            # Move batch to device\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            context_input_ids = batch['context_input_ids'].to(device)\n            context_attention_mask = batch['context_attention_mask'].to(device)\n\n            # Print shapes of input tensors for debugging\n            print(\"Input IDs shape:\", input_ids.shape)\n            print(\"Attention mask shape:\", attention_mask.shape)\n            print(\"Context input IDs shape:\", context_input_ids.shape)\n            print(\"Context attention mask shape:\", context_attention_mask.shape)\n\n            # Check input shapes\n            try:\n                # Expected shape: [batch_size, sequence_length]\n                expected_shape = (input_ids.size(0), input_ids.size(1))  # (batch_size, seq_length)\n                assert input_ids.dim() == 2, \"Input IDs should be 2-dimensional\"\n                assert input_ids.shape == expected_shape, f\"Expected input_ids shape {expected_shape}, but got {input_ids.shape}\"\n\n                assert attention_mask.dim() == 2, \"Attention mask should be 2-dimensional\"\n                assert attention_mask.shape == expected_shape, f\"Expected attention_mask shape {expected_shape}, but got {attention_mask.shape}\"\n\n                assert context_input_ids.dim() == 2, \"Context input IDs should be 2-dimensional\"\n                assert context_input_ids.shape == expected_shape, f\"Expected context_input_ids shape {expected_shape}, but got {context_input_ids.shape}\"\n\n                assert context_attention_mask.dim() == 2, \"Context attention mask should be 2-dimensional\"\n                assert context_attention_mask.shape == expected_shape, f\"Expected context_attention_mask shape {expected_shape}, but got {context_attention_mask.shape}\"\n\n                # Forward pass\n                outputs = model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    context_input_ids=context_input_ids,\n                    context_attention_mask=context_attention_mask,\n                    labels=context_input_ids\n                )\n\n                # Process loss and perform backpropagation\n                loss = outputs.loss\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                epoch_loss += loss.item()\n\n            except AssertionError as e:\n                print(f\"Shape Assertion Error: {e}\")\n            except Exception as e:\n                print(f\"Error during forward pass: {e}\")\n\n        print(f\"Epoch {epoch + 1} completed. Loss: {epoch_loss / len(train_loader)}\")","metadata":{"id":"LA6PevHNc4Zg","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Full training script with updated function\n\nif __name__ == '__main__':\n    # Assuming train_df is already defined and preprocessed\n    train_dataset = LegalDataset(\n        df=train_df,\n        tokenizer_question=dpr_question_tokenizer,\n        tokenizer_context=dpr_context_tokenizer,\n        max_length=128  # Maximum token length for truncation\n    )\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=8,\n        shuffle=True,\n        num_workers=4\n    )\n\n    retriever = FaissRetriever(index=index, passages=passages, tokenizer=dpr_question_tokenizer, top_k=5)\n    model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever).to(device)\n    free_memory\n    optimizer = AdamW(model.parameters(), lr=5e-5)\n\n    model.train()\n    # Train the model\n    train_model(model, train_loader, optimizer, device, num_epochs=200)\n\n    # Save the fine-tuned model\n    model.save_pretrained('fine_tuned_rag_model')\n    print(\"Fine-tuning completed and model saved.\")","metadata":{"id":"YadEbkoVc6Ep","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction","metadata":{"id":"wEx1HqoMc9nq"}},{"cell_type":"code","source":"# Step 9: Loading the fine-tuned model\nprint(\"Loading the fine-tuned model...\")\nretriever = FaissRetriever(index=index, passages=passages, tokenizer=dpr_question_tokenizer, top_k=10)\nfine_tuned_model = RagTokenForGeneration.from_pretrained('fine_tuned_rag_model', retriever=retriever).to(device)\nprint(\"Fine-tuned model loaded successfully.\")\n# Step 8: Predict top-k documents for each query in public_test.csv with progress bar\ntop_k_predictions = []\nfine_tuned_model.eval()\n# Use tqdm to display progress while generating predictions\nfor idx, row in tqdm(public_test_df.iterrows(), total=len(public_test_df), desc=\"Processing Queries\"):\n    question = row['question']\n    qid = row['qid']\n\n    # Tokenize question\n    input_ids = dpr_question_tokenizer(question, return_tensors=\"pt\", max_length=128, truncation=True, padding=True).input_ids\n\n    # Move inputs to the MPS\n    input_ids = input_ids.to(device)\n\n    # Generate top-k passage predictions\n    retrieved_passages = retriever.retrieve(input_ids, None)\n\n    # Retrieve the passage IDs (cid) for the predicted passages\n    retrieved_cids = [corpus_df.loc[corpus_df['text'] == passage]['cid'].values[0] for passage in retrieved_passages]\n\n    # Append result (qid followed by top-k cids)\n    top_k_predictions.append(f\"{qid} \" + \" \".join(map(str, retrieved_cids)))\n\n    # Free MPS memory after each query\n    free_memory()\n\n# Step 9: Save predictions to predict.txt\nwith open('predict.txt', 'w') as f:\n    for prediction in top_k_predictions:\n        f.write(prediction + '\\n')\n\nprint(\"Predictions saved to predict.txt.\")","metadata":{"id":"Z7nWSDD1c_T9","trusted":true},"execution_count":null,"outputs":[]}]}