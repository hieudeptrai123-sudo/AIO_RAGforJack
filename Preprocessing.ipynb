{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyvi import ViTokenizer\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "import faiss\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('corpus.csv')\n",
    "public_test = pd.read_csv('public_test.csv')\n",
    "train = pd.read_csv('train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NaN values and reset index of corpus dataframe\n",
    "corpus = corpus.dropna().reset_index(drop=True)\n",
    "\n",
    "# Remove rows with NaN values and reset index of public_test dataframe\n",
    "public_test = public_test.dropna().reset_index(drop=True)\n",
    "\n",
    "# Remove rows with NaN values and reset index of train dataframe\n",
    "train = train.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['text'] = corpus['text'].apply(lambda x: x.replace('\\n', ' '))\n",
    "corpus['text'] = corpus['text'].apply(lambda x: x.replace('\\t', ' '))\n",
    "corpus['text'] = corpus['text'].apply(lambda x: x.replace('\\r', ' '))\n",
    "corpus['text'] = corpus['text'].apply(lambda x: x.replace('  ', ' '))\n",
    "corpus['text'] = corpus['text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Đường dẫn đến hai file .txt\n",
    "file1_path = 'vietnamese-stopwords-dash.txt'  # Thay đổi đường dẫn tới file thứ nhất\n",
    "file2_path = 'vietnamese-stopwords.txt'  # Thay đổi đường dẫn tới file thứ hai\n",
    "def read_stopwords(file_path):\n",
    "    stopwords = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            stopword = line.strip()  # Tách theo khoảng trắng\n",
    "            stopwords.append(stopword)  # Thêm stopword vào list\n",
    "    return stopwords\n",
    "stopwords1 = read_stopwords(file1_path)\n",
    "stopwords2 = read_stopwords(file2_path)\n",
    "\n",
    "vietnamese_stopwords = set(stopwords1 + stopwords2)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    no_punct_text = re.sub(r'[^\\w\\s]', '', text)  # This will keep Vietnamese letters and spaces\n",
    "    return no_punct_text\n",
    "\n",
    "corpus['text'] = corpus['text'].apply(lambda x: ViTokenizer.tokenize(x))\n",
    "\n",
    "corpus['text'] = corpus['text'].apply(lambda x: remove_punctuation(x))\n",
    "\n",
    "corpus['text'] = corpus['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in vietnamese_stopwords]))\n",
    "\n",
    "corpus['text'] = corpus['text'].apply(lambda x: unidecode.unidecode(x))\n",
    "public_test['question'] = public_test['question'].apply(lambda x: x.replace('\\n', ' '))\n",
    "public_test['question'] = public_test['question'].apply(lambda x: x.replace('\\t', ' '))\n",
    "public_test['question'] = public_test['question'].apply(lambda x: x.replace('\\r', ' '))\n",
    "public_test['question'] = public_test['question'].apply(lambda x: x.replace('  ', ' '))\n",
    "public_test['question'] = public_test['question'].apply(lambda x: x.lower())\n",
    "\n",
    "# Đường dẫn đến hai file .txt\n",
    "file1_path = 'vietnamese-stopwords-dash.txt'  # Thay đổi đường dẫn tới file thứ nhất\n",
    "file2_path = 'vietnamese-stopwords.txt'  # Thay đổi đường dẫn tới file thứ hai\n",
    "def read_stopwords(file_path):\n",
    "    stopwords = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            stopword = line.strip()  # Tách theo khoảng trắng\n",
    "            stopwords.append(stopword)  # Thêm stopword vào list\n",
    "    return stopwords\n",
    "stopwords1 = read_stopwords(file1_path)\n",
    "stopwords2 = read_stopwords(file2_path)\n",
    "\n",
    "vietnamese_stopwords = set(stopwords1 + stopwords2)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    no_punct_text = re.sub(r'[^\\w\\s]', '', text)  # This will keep Vietnamese letters and spaces\n",
    "    return no_punct_text\n",
    "\n",
    "public_test['question'] = public_test['question'].apply(lambda x: ViTokenizer.tokenize(x))\n",
    "\n",
    "public_test['question'] = public_test['question'].apply(lambda x: remove_punctuation(x))\n",
    "\n",
    "public_test['question'] = public_test['question'].apply(lambda x: ' '.join([word for word in x.split() if word not in vietnamese_stopwords]))\n",
    "\n",
    "public_test['question'] = public_test['question'].apply(lambda x: unidecode.unidecode(x))\n",
    "train['question'] = train['question'].apply(lambda x: x.replace('\\n', ' '))\n",
    "train['question'] = train['question'].apply(lambda x: x.replace('\\t', ' '))\n",
    "train['question'] = train['question'].apply(lambda x: x.replace('\\r', ' '))\n",
    "train['question'] = train['question'].apply(lambda x: x.replace('  ', ' '))\n",
    "train['question'] = train['question'].apply(lambda x: x.lower())\n",
    "\n",
    "# Đường dẫn đến hai file .txt\n",
    "file1_path = 'vietnamese-stopwords-dash.txt'  # Thay đổi đường dẫn tới file thứ nhất\n",
    "file2_path = 'vietnamese-stopwords.txt'  # Thay đổi đường dẫn tới file thứ hai\n",
    "def read_stopwords(file_path):\n",
    "    stopwords = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            stopword = line.strip()  # Tách theo khoảng trắng\n",
    "            stopwords.append(stopword)  # Thêm stopword vào list\n",
    "    return stopwords\n",
    "stopwords1 = read_stopwords(file1_path)\n",
    "stopwords2 = read_stopwords(file2_path)\n",
    "\n",
    "vietnamese_stopwords = set(stopwords1 + stopwords2)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    no_punct_text = re.sub(r'[^\\w\\s]', '', text)  # This will keep Vietnamese letters and spaces\n",
    "    return no_punct_text\n",
    "\n",
    "train['question'] = train['question'].apply(lambda x: ViTokenizer.tokenize(x))\n",
    "\n",
    "train['question'] = train['question'].apply(lambda x: remove_punctuation(x))\n",
    "\n",
    "train['question'] = train['question'].apply(lambda x: ' '.join([word for word in x.split() if word not in vietnamese_stopwords]))\n",
    "\n",
    "train['question'] = train['question'].apply(lambda x: unidecode.unidecode(x))\n",
    "train['context'] = train['context'].apply(lambda x: x.replace('\\n', ' '))\n",
    "train['context'] = train['context'].apply(lambda x: x.replace('\\t', ' '))\n",
    "train['context'] = train['context'].apply(lambda x: x.replace('\\r', ' '))\n",
    "train['context'] = train['context'].apply(lambda x: x.replace('  ', ' '))\n",
    "train['context'] = train['context'].apply(lambda x: x.lower())\n",
    "\n",
    "# Đường dẫn đến hai file .txt\n",
    "file1_path = 'vietnamese-stopwords-dash.txt'  # Thay đổi đường dẫn tới file thứ nhất\n",
    "file2_path = 'vietnamese-stopwords.txt'  # Thay đổi đường dẫn tới file thứ hai\n",
    "def read_stopwords(file_path):\n",
    "    stopwords = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            stopword = line.strip()  # Tách theo khoảng trắng\n",
    "            stopwords.append(stopword)  # Thêm stopword vào list\n",
    "    return stopwords\n",
    "stopwords1 = read_stopwords(file1_path)\n",
    "stopwords2 = read_stopwords(file2_path)\n",
    "\n",
    "vietnamese_stopwords = set(stopwords1 + stopwords2)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    no_punct_text = re.sub(r'[^\\w\\s]', '', text)  # This will keep Vietnamese letters and spaces\n",
    "    return no_punct_text\n",
    "\n",
    "train['context'] = train['context'].apply(lambda x: ViTokenizer.tokenize(x))\n",
    "\n",
    "train['context'] = train['context'].apply(lambda x: remove_punctuation(x))\n",
    "\n",
    "train['context'] = train['context'].apply(lambda x: ' '.join([word for word in x.split() if word not in vietnamese_stopwords]))\n",
    "\n",
    "train['context'] = train['context'].apply(lambda x: unidecode.unidecode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train.csv', index=False)\n",
    "public_test.to_csv('public_test.csv', index=False)\n",
    "corpus.to_csv('corpus.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import RagRetriever, RagTokenForGeneration, DPRContextEncoder, DPRQuestionEncoder, DPRContextEncoderTokenizer, DPRQuestionEncoderTokenizer\n",
    "import faiss\n",
    "\n",
    "# Check if MPS is available\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Step 1: Load your dataset\n",
    "corpus_df = pd.read_csv('corpus.csv')  # Assuming columns: 'context' and 'cid'\n",
    "train_df = pd.read_csv('train.csv')  # Assuming columns: 'question', 'context', 'cid', 'qid'\n",
    "public_test_df = pd.read_csv('public_test.csv')  # Assuming columns: 'question', 'qid'\n",
    "\n",
    "# Step 2: Extract passages and their IDs\n",
    "passages = corpus_df['text'].tolist()\n",
    "passage_ids = corpus_df['cid'].tolist()\n",
    "\n",
    "# Step 3: Load DPR context and question encoders with appropriate tokenizers\n",
    "dpr_context_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\", ignore_mismatched_sizes=True).to(device)\n",
    "dpr_context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "dpr_question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\").to(device)\n",
    "dpr_question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "\n",
    "# Step 4: Embed all passages using DPRContextEncoder\n",
    "def embed_passages(passages, batch_size=32):\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Wrap the for loop with tqdm to display the progress\n",
    "    for i in tqdm(range(0, len(passages), batch_size), desc=\"Embedding Passages\"):\n",
    "        batch = passages[i:i + batch_size]\n",
    "        inputs = dpr_context_tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "        \n",
    "        # Move inputs to the MPS\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        \n",
    "        # Disable gradient computation for faster inference\n",
    "        with torch.no_grad():\n",
    "            embeddings = dpr_context_encoder(**inputs).pooler_output\n",
    "        \n",
    "        all_embeddings.append(embeddings)\n",
    "    \n",
    "    # Concatenate all the embeddings into a single tensor\n",
    "    return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "# Get embeddings for all passages\n",
    "passage_embeddings = embed_passages(passages)\n",
    "\n",
    "# Step 5: Use FAISS to index the passage embeddings\n",
    "d = passage_embeddings.shape[1]  # Dimensionality of the embeddings\n",
    "\n",
    "# Create a FAISS index for MPS\n",
    "res = faiss.StandardGpuResources()  # Create MPS resources\n",
    "index = faiss.IndexFlatIP(d)  # Inner Product (dot product) index on CPU\n",
    "mps_index = faiss.index_cpu_to_gpu(res, 0, index)  # Move the index to MPS\n",
    "mps_index.add(passage_embeddings.cpu().numpy())  # Add embeddings to the index (move to CPU for FAISS)\n",
    "\n",
    "# Step 6: Create a custom retriever using FAISS\n",
    "class FaissRetriever(RagRetriever):\n",
    "    def __init__(self, index, passages, tokenizer, top_k=5):\n",
    "        self.index = index\n",
    "        self.passages = passages\n",
    "        self.tokenizer = tokenizer\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def retrieve(self, question_input_ids, question_hidden_states, question_attention_mask=None):\n",
    "        # Tokenize and embed the question using DPRQuestionEncoder\n",
    "        question_embeds = self._embed_question(question_input_ids)\n",
    "        \n",
    "        # Search FAISS index for top-k relevant documents\n",
    "        _, indices = self.index.search(question_embeds.cpu().numpy(), self.top_k)  # Move to CPU for FAISS\n",
    "        \n",
    "        # Return top-k passages\n",
    "        retrieved_passages = [self.passages[i] for i in indices[0]]\n",
    "        return retrieved_passages\n",
    "\n",
    "    def _embed_question(self, question_input_ids):\n",
    "        question = self.tokenizer.batch_decode(question_input_ids, skip_special_tokens=True)\n",
    "        inputs = dpr_question_tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "        \n",
    "        # Move inputs to the MPS\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            question_embeddings = dpr_question_encoder(**inputs).pooler_output\n",
    "        return question_embeddings\n",
    "\n",
    "# Step 7: Initialize the custom retriever and the RAG model\n",
    "retriever = FaissRetriever(index=mps_index, passages=passages, tokenizer=dpr_question_tokenizer, top_k=10)\n",
    "model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever).to(device)\n",
    "\n",
    "# Step 8: Predict top-k documents for each query in public_test.csv with progress bar\n",
    "top_k_predictions = []\n",
    "\n",
    "# Use tqdm to display progress while generating predictions\n",
    "for idx, row in tqdm(public_test_df.iterrows(), total=len(public_test_df), desc=\"Processing Queries\"):\n",
    "    question = row['question']\n",
    "    qid = row['qid']\n",
    "    \n",
    "    # Tokenize question\n",
    "    input_ids = dpr_question_tokenizer(question, return_tensors=\"pt\", max_length=128, truncation=True, padding=True).input_ids\n",
    "    \n",
    "    # Move inputs to the MPS\n",
    "    input_ids = input_ids.to(device)\n",
    "    \n",
    "    # Generate top-k passage predictions\n",
    "    retrieved_passages = retriever.retrieve(input_ids, None)\n",
    "    \n",
    "    # Retrieve the passage IDs (cid) for the predicted passages\n",
    "    retrieved_cids = [corpus_df.loc[corpus_df['context'] == passage]['cid'].values[0] for passage in retrieved_passages]\n",
    "    \n",
    "    # Append result (qid followed by top-k cids)\n",
    "    top_k_predictions.append(f\"{qid} \" + \" \".join(map(str, retrieved_cids)))\n",
    "\n",
    "# Step 9: Save predictions to predict.txt\n",
    "with open('predict.txt', 'w') as f:\n",
    "    for prediction in top_k_predictions:\n",
    "        f.write(prediction + '\\n')\n",
    "\n",
    "print(\"Predictions saved to predict.txt.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
