{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MuyuWTL6gGRy"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from pyvi import ViTokenizer\n","import re\n","import unidecode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SeeEbhaygGR2"},"outputs":[],"source":["corpus = pd.read_csv('corpus.csv')\n","public_test = pd.read_csv('public_test.csv')\n","train = pd.read_csv('train.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6oC361-JgGR3"},"outputs":[],"source":["corpus['text'] = corpus['text'].apply(lambda x: x.replace('\\n', ' '))\n","corpus['text'] = corpus['text'].apply(lambda x: x.replace('\\t', ' '))\n","corpus['text'] = corpus['text'].apply(lambda x: x.replace('\\r', ' '))\n","corpus['text'] = corpus['text'].apply(lambda x: x.replace('  ', ' '))\n","corpus['text'] = corpus['text'].apply(lambda x: x.lower())\n","\n","# Đường dẫn đến hai file .txt\n","file1_path = 'vietnamese-stopwords-dash.txt'  # Thay đổi đường dẫn tới file thứ nhất\n","file2_path = 'vietnamese-stopwords.txt'  # Thay đổi đường dẫn tới file thứ hai\n","def read_stopwords(file_path):\n","    stopwords = []\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        for line in file:\n","            stopword = line.strip()  # Tách theo khoảng trắng\n","            stopwords.append(stopword)  # Thêm stopword vào list\n","    return stopwords\n","stopwords1 = read_stopwords(file1_path)\n","stopwords2 = read_stopwords(file2_path)\n","\n","vietnamese_stopwords = set(stopwords1 + stopwords2)\n","\n","def remove_punctuation(text):\n","    no_punct_text = re.sub(r'[^\\w\\s]', '', text)  # This will keep Vietnamese letters and spaces\n","    return no_punct_text\n","\n","corpus['text'] = corpus['text'].apply(lambda x: ViTokenizer.tokenize(x))\n","\n","corpus['text'] = corpus['text'].apply(lambda x: remove_punctuation(x))\n","\n","corpus['text'] = corpus['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in vietnamese_stopwords]))\n","\n","corpus['text'] = corpus['text'].apply(lambda x: unidecode.unidecode(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b_DFB3CjgGR4"},"outputs":[],"source":["public_test['question'] = public_test['question'].apply(lambda x: x.replace('\\n', ' '))\n","public_test['question'] = public_test['question'].apply(lambda x: x.replace('\\t', ' '))\n","public_test['question'] = public_test['question'].apply(lambda x: x.replace('\\r', ' '))\n","public_test['question'] = public_test['question'].apply(lambda x: x.replace('  ', ' '))\n","public_test['question'] = public_test['question'].apply(lambda x: x.lower())\n","\n","# Đường dẫn đến hai file .txt\n","file1_path = 'vietnamese-stopwords-dash.txt'  # Thay đổi đường dẫn tới file thứ nhất\n","file2_path = 'vietnamese-stopwords.txt'  # Thay đổi đường dẫn tới file thứ hai\n","def read_stopwords(file_path):\n","    stopwords = []\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        for line in file:\n","            stopword = line.strip()  # Tách theo khoảng trắng\n","            stopwords.append(stopword)  # Thêm stopword vào list\n","    return stopwords\n","stopwords1 = read_stopwords(file1_path)\n","stopwords2 = read_stopwords(file2_path)\n","\n","vietnamese_stopwords = set(stopwords1 + stopwords2)\n","\n","def remove_punctuation(text):\n","    no_punct_text = re.sub(r'[^\\w\\s]', '', text)  # This will keep Vietnamese letters and spaces\n","    return no_punct_text\n","\n","public_test['question'] = public_test['question'].apply(lambda x: ViTokenizer.tokenize(x))\n","\n","public_test['question'] = public_test['question'].apply(lambda x: remove_punctuation(x))\n","\n","public_test['question'] = public_test['question'].apply(lambda x: ' '.join([word for word in x.split() if word not in vietnamese_stopwords]))\n","\n","public_test['question'] = public_test['question'].apply(lambda x: unidecode.unidecode(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lg-mUX9jgGR4"},"outputs":[],"source":["train['question'] = train['question'].apply(lambda x: x.replace('\\n', ' '))\n","train['question'] = train['question'].apply(lambda x: x.replace('\\t', ' '))\n","train['question'] = train['question'].apply(lambda x: x.replace('\\r', ' '))\n","train['question'] = train['question'].apply(lambda x: x.replace('  ', ' '))\n","train['question'] = train['question'].apply(lambda x: x.lower())\n","\n","# Đường dẫn đến hai file .txt\n","file1_path = 'vietnamese-stopwords-dash.txt'  # Thay đổi đường dẫn tới file thứ nhất\n","file2_path = 'vietnamese-stopwords.txt'  # Thay đổi đường dẫn tới file thứ hai\n","def read_stopwords(file_path):\n","    stopwords = []\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        for line in file:\n","            stopword = line.strip()  # Tách theo khoảng trắng\n","            stopwords.append(stopword)  # Thêm stopword vào list\n","    return stopwords\n","stopwords1 = read_stopwords(file1_path)\n","stopwords2 = read_stopwords(file2_path)\n","\n","vietnamese_stopwords = set(stopwords1 + stopwords2)\n","\n","def remove_punctuation(text):\n","    no_punct_text = re.sub(r'[^\\w\\s]', '', text)  # This will keep Vietnamese letters and spaces\n","    return no_punct_text\n","\n","train['question'] = train['question'].apply(lambda x: ViTokenizer.tokenize(x))\n","\n","train['question'] = train['question'].apply(lambda x: remove_punctuation(x))\n","\n","train['question'] = train['question'].apply(lambda x: ' '.join([word for word in x.split() if word not in vietnamese_stopwords]))\n","\n","train['question'] = train['question'].apply(lambda x: unidecode.unidecode(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jNUvS3PggGR5"},"outputs":[],"source":["train['context'] = train['context'].apply(lambda x: x.replace('\\n', ' '))\n","train['context'] = train['context'].apply(lambda x: x.replace('\\t', ' '))\n","train['context'] = train['context'].apply(lambda x: x.replace('\\r', ' '))\n","train['context'] = train['context'].apply(lambda x: x.replace('  ', ' '))\n","train['context'] = train['context'].apply(lambda x: x.lower())\n","\n","# Đường dẫn đến hai file .txt\n","file1_path = 'vietnamese-stopwords-dash.txt'  # Thay đổi đường dẫn tới file thứ nhất\n","file2_path = 'vietnamese-stopwords.txt'  # Thay đổi đường dẫn tới file thứ hai\n","def read_stopwords(file_path):\n","    stopwords = []\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        for line in file:\n","            stopword = line.strip()  # Tách theo khoảng trắng\n","            stopwords.append(stopword)  # Thêm stopword vào list\n","    return stopwords\n","stopwords1 = read_stopwords(file1_path)\n","stopwords2 = read_stopwords(file2_path)\n","\n","vietnamese_stopwords = set(stopwords1 + stopwords2)\n","\n","def remove_punctuation(text):\n","    no_punct_text = re.sub(r'[^\\w\\s]', '', text)  # This will keep Vietnamese letters and spaces\n","    return no_punct_text\n","\n","train['context'] = train['context'].apply(lambda x: ViTokenizer.tokenize(x))\n","\n","train['context'] = train['context'].apply(lambda x: remove_punctuation(x))\n","\n","train['context'] = train['context'].apply(lambda x: ' '.join([word for word in x.split() if word not in vietnamese_stopwords]))\n","\n","train['context'] = train['context'].apply(lambda x: unidecode.unidecode(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QIquZJG_gGR5"},"outputs":[],"source":["train.to_csv('train.csv', index=False)\n","public_test.to_csv('public_test.csv', index=False)\n","corpus.to_csv('corpus.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dhF_C6DFgGR5","outputId":"6950c860-73d2-4fad-b33a-a8fe8b424ea1"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question</th>\n","      <th>context</th>\n","      <th>cid</th>\n","      <th>qid</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>hoc nganh quan_ly khai_thac cong_trinh thuy_lo...</td>\n","      <td>kha_nang hoc_tap nang cao_trinh_do n khoi_luon...</td>\n","      <td>[62492]</td>\n","      <td>161615</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>noi_dung long_ghep binh_dang gioi xay_dung van...</td>\n","      <td>noi_dung long_ghep binh_dang gioi xay_dung van...</td>\n","      <td>[151154]</td>\n","      <td>80037</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>san_pham phan_mem huong uu_dai mien thue thue ...</td>\n","      <td>20 uu_dai mien thue thue n1 mien thue bon 50 t...</td>\n","      <td>[75071]</td>\n","      <td>124074</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>giao_vien co_so giao_duc mam_non tieu_hoc cong...</td>\n","      <td>huong ncan quan_ly giao_vien nhan_vien huong c...</td>\n","      <td>[225897]</td>\n","      <td>146841</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>nguyen_tac ap_dung phu_cap uu_dai nghe y_te</td>\n","      <td>nguyen_tac ap_dung n1 truong_hop cong_chuc vie...</td>\n","      <td>[68365]</td>\n","      <td>6176</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            question  \\\n","0  hoc nganh quan_ly khai_thac cong_trinh thuy_lo...   \n","1  noi_dung long_ghep binh_dang gioi xay_dung van...   \n","2  san_pham phan_mem huong uu_dai mien thue thue ...   \n","3  giao_vien co_so giao_duc mam_non tieu_hoc cong...   \n","4        nguyen_tac ap_dung phu_cap uu_dai nghe y_te   \n","\n","                                             context       cid     qid  \n","0  kha_nang hoc_tap nang cao_trinh_do n khoi_luon...   [62492]  161615  \n","1  noi_dung long_ghep binh_dang gioi xay_dung van...  [151154]   80037  \n","2  20 uu_dai mien thue thue n1 mien thue bon 50 t...   [75071]  124074  \n","3  huong ncan quan_ly giao_vien nhan_vien huong c...  [225897]  146841  \n","4  nguyen_tac ap_dung n1 truong_hop cong_chuc vie...   [68365]    6176  "]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["train.head(5)"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}